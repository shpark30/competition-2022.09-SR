{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed6d86-0a4c-46a4-91a5-545968d16979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hat.archs.hat_archs.HAT이 아닌 hat.archs.hat_model.HATModel을 학습에서 사용. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522c8e00-738b-4526-9ca3-1dbb492e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from basicsr.data.transforms import augment, paired_random_crop\n",
    "import os\n",
    "os.chdir('HAT_official/hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0eec86-6bc2-46f8-b5ef-9c8768b93c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/competition/2022.09_SR/HAT_official/hat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a1b4e2-2698-4437-b1d3-941eb9b85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from archs.hat_arch import HAT\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58e6ef-5e9e-4112-90af-a5496d3c7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_to_tensor(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "    return img\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 lr_path,\n",
    "                 hr_path,\n",
    "                 train_ratio=0.8,\n",
    "                 mode='train',\n",
    "                 seed=2022,\n",
    "                 transform=A.Compose([\n",
    "                     A.HorizontalFlip(p=0.5),\n",
    "                     A.Rotate(limit=180, p=0.9),\n",
    "                    ], additional_targets={'image2': 'image'}),\n",
    "                 augmentation_prop=0.5):\n",
    "        \n",
    "        random.seed(2022)\n",
    "        self.lr_path = lr_path\n",
    "        self.hr_path = hr_path\n",
    "        \n",
    "        img_list = os.listdir(lr_path)\n",
    "        random.shuffle(img_list)\n",
    "        if mode=='train':\n",
    "            self.img_list = img_list[:round(len(img_list)*train_ratio)]\n",
    "        elif mode=='valid':\n",
    "            self.img_list = img_list[round(len(img_list)*train_ratio):]\n",
    "        else:\n",
    "            raise f'invalid mode. {mode}'\n",
    "            \n",
    "        self.transform = transform\n",
    "        self.augmentation_prop = augmentation_prop\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr = cv2.imread(os.path.join(self.lr_path, self.img_list[idx]), cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "        hr = cv2.imread(os.path.join(self.hr_path, self.img_list[idx]), cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "        \n",
    "        p = random.random()\n",
    "        if p < self.augmentation_prop and self.mode=='train':\n",
    "            gt_size = self.opt['gt_size']\n",
    "            # random crop\n",
    "            hr, lr = paired_random_crop(hr, lr, gt_size=480, scale=4)\n",
    "            # flip, rotation\n",
    "            hr, lr = augment([hr, lr], hflip=True, rotation=True)\n",
    "            \n",
    "            # augmentations = self.transform(image=lr, image2=hr)\n",
    "            # lr, hr = augmentations['image'], augmentations['image2']\n",
    "            \n",
    "        lr = torch.from_numpy(np.transpose(lr[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "        hr = torch.from_numpy(np.transpose(hr[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "        \n",
    "        return lr, hr \n",
    "    \n",
    "lr_path = 'HAT_official/datasets/data/train/120_480/lr'\n",
    "hr_path = 'HAT_official/datasets/data/train/120_480/hr'\n",
    "train_set = SRDataset(lr_path=lr_path, hr_path=hr_path, train_ratio=0.9, mode='train')\n",
    "valid_set = SRDataset(lr_path=lr_path, hr_path=hr_path, train_ratio=0.9, mode='valid')\n",
    "print(len(train_set), len(valid_set))\n",
    "\n",
    "batch_size=1\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=6)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72292f01-233a-4591-b67f-3010b015838c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d218edd-35ab-4986-8a3f-91085d4513f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HAT(upscale=4,\n",
    "            in_chans=3,\n",
    "            img_size=64,\n",
    "            window_size=16,\n",
    "            compress_ratio=3,\n",
    "            squeeze_factor=30,\n",
    "            conv_scale=0.01,\n",
    "            overlap_ratio=0.5,\n",
    "            img_range=1.,\n",
    "            depths=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            embed_dim=180,\n",
    "            num_heads=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            mlp_ratio=2,\n",
    "            upsampler='pixelshuffle',\n",
    "            resi_connection='1conv')\n",
    "pretrained_path = 'HAT_official/experiments/pretrained_models/HAT-L_SRx4_ImageNet-pretrain.pth'\n",
    "pretrained_states = torch.load(pretrained_path)\n",
    "model.load_state_dict(pretrained_states['params_ema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b69412-7fc8-4f33-b05c-54346beee16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.99), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96189fc0-b61e-42ce-b3ca-5a1cc5e2abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNR:\n",
    "    \"\"\"Peak Signal to Noise Ratio\n",
    "    img1 and img2 have range [0, 255]\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"PSNR\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(img1, img2):\n",
    "        mse = torch.mean((img1 - img2) ** 2)\n",
    "        return 20 * torch.log10(255.0 / torch.sqrt(mse))\n",
    "metric=PSNR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aa30c-c2c2-4d8f-ad5d-2488d1fd4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=64\n",
    "scale=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658a972-09d1-49c7-b1a6-0a465cdd8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e2e16-aca3-4462-8888-e84697b630a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae1345-3711-4922-8594-c0a0cb26b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "epochs = 10\n",
    "total_iter = 10000\n",
    "i = 0\n",
    "while i < total_iter:\n",
    "    for lr, hr in train_loader:\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        \n",
    "        # padding\n",
    "        mod_pad_h, mod_pad_w = 0, 0\n",
    "        _, _, h, w = lr.size()\n",
    "        if h % window_size != 0:\n",
    "            mod_pad_h = window_size - h % window_size\n",
    "        if w % window_size != 0:\n",
    "            mod_pad_w = window_size - w % window_size\n",
    "        lr = F.pad(lr, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        \n",
    "        with torch.cuda.amp.autocast():  \n",
    "            sr = model(lr)\n",
    "        \n",
    "            # padding한 부분 삭제\n",
    "            _, _, h, w = sr.size()\n",
    "            sr = sr[:, :, 0:h - mod_pad_h * scale, 0:w - mod_pad_w * scale]\n",
    "\n",
    "            loss = loss_fn(sr, hr)\n",
    "        \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(scaler)\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        scaler.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance\n",
    "        \n",
    "        prog_bar.set_description(round(float(loss), 4))\n",
    "        \n",
    "        i += 1\n",
    "        if i%1==0:\n",
    "            print(i, round(loss.data.float(), 4))\n",
    "        \n",
    "        # validation\n",
    "        if i%2==0:\n",
    "            model.eval()\n",
    "            running_loss = 0\n",
    "            running_psnr = 0\n",
    "            for lr, hr in valid_loader:\n",
    "                with torch.no_grad():\n",
    "                    lr, hr = lr.to(device), hr.to(device)\n",
    "                    \n",
    "                    # pad\n",
    "                    mod_pad_h, mod_pad_w = 0, 0\n",
    "                    _, _, h, w = lr.size()\n",
    "                    if h % window_size != 0:\n",
    "                        mod_pad_h = window_size - h % window_size\n",
    "                    if w % window_size != 0:\n",
    "                        mod_pad_w = window_size - w % window_size\n",
    "                    lr = F.pad(lr, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\n",
    "                    sr = model(lr)\n",
    "                    # padding한 부분 삭제\n",
    "                    _, _, h, w = sr.size()\n",
    "                    sr = sr[:, :, 0:h - mod_pad_h * scale, 0:w - mod_pad_w * scale]\n",
    "                    \n",
    "                    running_loss += loss_fn(sr, hr) * sr.shape[0]\n",
    "                    for bi in range(sr.shape[0]):\n",
    "                        running_psnr = metric(sr[0], hr[0])\n",
    "            print(i, f'valid | loss {round(float(running_loss), 4)} | psnr {round(float(running_psnr, 4))}')\n",
    "            torch.save({'params_ema': model.state_dict()}, f'experiments/1007/HAT-L_120-960_{i}.pth')\n",
    "            model.train()\n",
    "                    \n",
    "        if i==total_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b8096-a5e4-41eb-9167-6e2079bc980e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c98ea-fba5-4e72-860b-b6ddf31a4351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52712d3-bae2-4fce-bbc5-78d682abbd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a15495-4098-4c17-827d-5ef4e8bd2c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352464346/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = HAT(upscale=4,\n",
    "            in_chans=3,\n",
    "            img_size=64,\n",
    "            window_size=16,\n",
    "            compress_ratio=3,\n",
    "            squeeze_factor=30,\n",
    "            conv_scale=0.01,\n",
    "            overlap_ratio=0.5,\n",
    "            img_range=1.,\n",
    "            depths=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            embed_dim=180,\n",
    "            num_heads=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            mlp_ratio=2,\n",
    "            upsampler='pixelshuffle',\n",
    "            resi_connection='1conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab89ba9-7902-497d-9ab0-8a64f3ec9fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'feed_data' in dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064dcdf-dc00-418d-805e-be02c6a4a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_options(root_path, is_train=True):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-opt', type=str, required=True, help='Path to option YAML file.')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none', help='job launcher')\n",
    "    parser.add_argument('--auto_resume', action='store_true')\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    parser.add_argument(\n",
    "        '--force_yml', nargs='+', default=None, help='Force to update yml files. Examples: train:ema_decay=0.999')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # parse yml to dict\n",
    "    opt = yaml_load(args.opt)\n",
    "\n",
    "    # distributed settings\n",
    "    if args.launcher == 'none':\n",
    "        opt['dist'] = False\n",
    "        print('Disable distributed.', flush=True)\n",
    "    else:\n",
    "        opt['dist'] = True\n",
    "        if args.launcher == 'slurm' and 'dist_params' in opt:\n",
    "            init_dist(args.launcher, **opt['dist_params'])\n",
    "        else:\n",
    "            init_dist(args.launcher)\n",
    "    opt['rank'], opt['world_size'] = get_dist_info()\n",
    "\n",
    "    # random seed\n",
    "    seed = opt.get('manual_seed')\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "        opt['manual_seed'] = seed\n",
    "    set_random_seed(seed + opt['rank'])\n",
    "\n",
    "    # force to update yml options\n",
    "    if args.force_yml is not None:\n",
    "        for entry in args.force_yml:\n",
    "            # now do not support creating new keys\n",
    "            keys, value = entry.split('=')\n",
    "            keys, value = keys.strip(), value.strip()\n",
    "            value = _postprocess_yml_value(value)\n",
    "            eval_str = 'opt'\n",
    "            for key in keys.split(':'):\n",
    "                eval_str += f'[\"{key}\"]'\n",
    "            eval_str += '=value'\n",
    "            # using exec function\n",
    "            exec(eval_str)\n",
    "\n",
    "    opt['auto_resume'] = args.auto_resume\n",
    "    opt['is_train'] = is_train\n",
    "\n",
    "    # debug setting\n",
    "    if args.debug and not opt['name'].startswith('debug'):\n",
    "        opt['name'] = 'debug_' + opt['name']\n",
    "\n",
    "    if opt['num_gpu'] == 'auto':\n",
    "        opt['num_gpu'] = torch.cuda.device_count()\n",
    "\n",
    "    # datasets\n",
    "    for phase, dataset in opt['datasets'].items():\n",
    "        # for multiple datasets, e.g., val_1, val_2; test_1, test_2\n",
    "        phase = phase.split('_')[0]\n",
    "        dataset['phase'] = phase\n",
    "        if 'scale' in opt:\n",
    "            dataset['scale'] = opt['scale']\n",
    "        if dataset.get('dataroot_gt') is not None:\n",
    "            dataset['dataroot_gt'] = osp.expanduser(dataset['dataroot_gt'])\n",
    "        if dataset.get('dataroot_lq') is not None:\n",
    "            dataset['dataroot_lq'] = osp.expanduser(dataset['dataroot_lq'])\n",
    "\n",
    "    # paths\n",
    "    for key, val in opt['path'].items():\n",
    "        if (val is not None) and ('resume_state' in key or 'pretrain_network' in key):\n",
    "            opt['path'][key] = osp.expanduser(val)\n",
    "\n",
    "    if is_train:\n",
    "        experiments_root = osp.join(root_path, 'experiments', opt['name'])\n",
    "        opt['path']['experiments_root'] = experiments_root\n",
    "        opt['path']['models'] = osp.join(experiments_root, 'models')\n",
    "        opt['path']['training_states'] = osp.join(experiments_root, 'training_states')\n",
    "        opt['path']['log'] = experiments_root\n",
    "        opt['path']['visualization'] = osp.join(experiments_root, 'visualization')\n",
    "\n",
    "        # change some options for debug mode\n",
    "        if 'debug' in opt['name']:\n",
    "            if 'val' in opt:\n",
    "                opt['val']['val_freq'] = 8\n",
    "            opt['logger']['print_freq'] = 1\n",
    "            opt['logger']['save_checkpoint_freq'] = 8\n",
    "    else:  # test\n",
    "        results_root = osp.join(root_path, 'results', opt['name'])\n",
    "        opt['path']['results_root'] = results_root\n",
    "        opt['path']['log'] = results_root\n",
    "        opt['path']['visualization'] = osp.join(results_root, 'visualization')\n",
    "\n",
    "    return opt, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db55dc1-f48d-4834-a50b-c41685696430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name': 'model',\n",
       " '_obj_map': {'SRModel': basicsr.models.sr_model.SRModel,\n",
       "  'VideoBaseModel': basicsr.models.video_base_model.VideoBaseModel,\n",
       "  'EDVRModel': basicsr.models.edvr_model.EDVRModel,\n",
       "  'SwinIRModel': basicsr.models.swinir_model.SwinIRModel,\n",
       "  'HiFaceGANModel': basicsr.models.hifacegan_model.HiFaceGANModel,\n",
       "  'SRGANModel': basicsr.models.srgan_model.SRGANModel,\n",
       "  'StyleGAN2Model': basicsr.models.stylegan2_model.StyleGAN2Model,\n",
       "  'VideoGANModel': basicsr.models.video_gan_model.VideoGANModel,\n",
       "  'VideoRecurrentModel': basicsr.models.video_recurrent_model.VideoRecurrentModel,\n",
       "  'ESRGANModel': basicsr.models.esrgan_model.ESRGANModel,\n",
       "  'VideoRecurrentGANModel': basicsr.models.video_recurrent_gan_model.VideoRecurrentGANModel}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from basicsr.utils.registry import MODEL_REGISTRY\n",
    "MODEL_REGISTRY.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7219c7c-f29e-49da-be09-1b7e8ae2031c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/competition/2022.09_SR/HAT_official/hat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb55c975-03f6-456c-b0de-1595d72e195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicsr.models import build_model\n",
    "from basicsr.utils.options import parse_options\n",
    "from models import hat_model\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "\n",
    "def ordered_yaml():\n",
    "    \"\"\"Support OrderedDict for yaml.\n",
    "    Returns:\n",
    "        tuple: yaml Loader and Dumper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from yaml import CDumper as Dumper\n",
    "        from yaml import CLoader as Loader\n",
    "    except ImportError:\n",
    "        from yaml import Dumper, Loader\n",
    "\n",
    "    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n",
    "\n",
    "    def dict_representer(dumper, data):\n",
    "        return dumper.represent_dict(data.items())\n",
    "\n",
    "    def dict_constructor(loader, node):\n",
    "        return OrderedDict(loader.construct_pairs(node))\n",
    "\n",
    "    Dumper.add_representer(OrderedDict, dict_representer)\n",
    "    Loader.add_constructor(_mapping_tag, dict_constructor)\n",
    "    return Loader, Dumper\n",
    "\n",
    "def yaml_load(f):\n",
    "    \"\"\"Load yaml file or string.\n",
    "    Args:\n",
    "        f (str): File path or a python string.\n",
    "    Returns:\n",
    "        dict: Loaded dict.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, 'r') as f:\n",
    "            return yaml.load(f, Loader=ordered_yaml()[0])\n",
    "    else:\n",
    "        return yaml.load(f, Loader=ordered_yaml()[0])\n",
    "    \n",
    "root_path = '../options/train/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain.yml'\n",
    "opt = yaml_load(root_path)\n",
    "# opt, args = parse_options(root_path, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5348fe7-aaac-417b-ab40-fea3b6751359",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['is_train'] = True\n",
    "opt['dist'] = False\n",
    "opt['path']['pretrain_network_g'] = '../experiments/pretrained_models/HAT-L_SRx4_ImageNet-pretrain.pth'\n",
    "model = build_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b51843b-1866-4e5d-a5c6-44c000c44421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'feed_data' in dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa4a35f-ab1a-4ae2-b275-02b66c203258",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HAT' object has no attribute 'feed_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135080/4236541494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HAT' object has no attribute 'feed_data'"
     ]
    }
   ],
   "source": [
    "model.feed_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
