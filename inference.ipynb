{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee69796-eaba-4c51-9e84-9bb3530d1fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../HAT_official/datasets/data/test/lr/20007.png',\n",
       " '../HAT_official/datasets/data/test/lr/20010.png',\n",
       " '../HAT_official/datasets/data/test/lr/20004.png',\n",
       " '../HAT_official/datasets/data/test/lr/20014.png',\n",
       " '../HAT_official/datasets/data/test/lr/20002.png',\n",
       " '../HAT_official/datasets/data/test/lr/20012.png',\n",
       " '../HAT_official/datasets/data/test/lr/20001.png',\n",
       " '../HAT_official/datasets/data/test/lr/20013.png',\n",
       " '../HAT_official/datasets/data/test/lr/20008.png',\n",
       " '../HAT_official/datasets/data/test/lr/20005.png',\n",
       " '../HAT_official/datasets/data/test/lr/20009.png',\n",
       " '../HAT_official/datasets/data/test/lr/20011.png',\n",
       " '../HAT_official/datasets/data/test/lr/20015.png',\n",
       " '../HAT_official/datasets/data/test/lr/20000.png',\n",
       " '../HAT_official/datasets/data/test/lr/20017.png',\n",
       " '../HAT_official/datasets/data/test/lr/20006.png',\n",
       " '../HAT_official/datasets/data/test/lr/20016.png',\n",
       " '../HAT_official/datasets/data/test/lr/20003.png']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from arch import HAT\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_list = glob(\"HAT_official/datasets/data/test/lr/*.png\")\n",
    "img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c9d35-23b6-4b59-9346-020e9882e414",
   "metadata": {},
   "source": [
    "# 이미지 한장 통째로 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ee6a000-4cf9-4584-bc1a-006b379661e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../HAT_official/datasets/data/test/lr/20007.png',\n",
       " '../HAT_official/datasets/data/test/lr/20010.png',\n",
       " '../HAT_official/datasets/data/test/lr/20004.png',\n",
       " '../HAT_official/datasets/data/test/lr/20014.png',\n",
       " '../HAT_official/datasets/data/test/lr/20002.png',\n",
       " '../HAT_official/datasets/data/test/lr/20012.png',\n",
       " '../HAT_official/datasets/data/test/lr/20001.png',\n",
       " '../HAT_official/datasets/data/test/lr/20013.png',\n",
       " '../HAT_official/datasets/data/test/lr/20008.png',\n",
       " '../HAT_official/datasets/data/test/lr/20005.png',\n",
       " '../HAT_official/datasets/data/test/lr/20009.png',\n",
       " '../HAT_official/datasets/data/test/lr/20011.png',\n",
       " '../HAT_official/datasets/data/test/lr/20015.png',\n",
       " '../HAT_official/datasets/data/test/lr/20000.png',\n",
       " '../HAT_official/datasets/data/test/lr/20017.png',\n",
       " '../HAT_official/datasets/data/test/lr/20006.png',\n",
       " '../HAT_official/datasets/data/test/lr/20016.png',\n",
       " '../HAT_official/datasets/data/test/lr/20003.png']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf00b328-4c82-4808-8c92-a9714bc86d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:54<00:00,  6.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.41s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.41s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:55<00:00,  6.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_img_to_tensor(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "    return img\n",
    "\n",
    "def test(model_path, input_img_paths, device='cuda'):\n",
    "    \n",
    "    metric = PSNR()\n",
    "    model = HAT(upscale=4,\n",
    "            in_chans=3,\n",
    "            img_size=64,\n",
    "            window_size=16,\n",
    "            compress_ratio=3,\n",
    "            squeeze_factor=30,\n",
    "            conv_scale=0.01,\n",
    "            overlap_ratio=0.5,\n",
    "            img_range=1.,\n",
    "            depths=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            embed_dim=180,\n",
    "            num_heads=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            mlp_ratio=2,\n",
    "            upsampler='pixelshuffle',\n",
    "            resi_connection='1conv')\n",
    "    \n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict['params_ema'], strict=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    save_path = os.path.join('HAT_official/datasets/data/test/inference', model_path.split('/')[-1][:-4])\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    \n",
    "    for img_path in tqdm(input_imgs, total=len(input_imgs), ncols=100):\n",
    "        input_img = load_img_to_tensor(img_path)\n",
    "        input_img = input_img.unsqueeze(0).to(device)\n",
    "        \n",
    "        # hr_img = img_path.split('/')[-1]\n",
    "        # hr_img = cv2.imread(os.path.join('../HAT_official/datasets/data', hr_img), cv2.IMREAD_COLOR).astype(np.float32)\n",
    "        # hr_img = torch.from_numpy(np.transpose(hr_img[:, :, [2, 1, 0]], (2, 0, 1))).float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_img)\n",
    "            # scores.append(metric(hr_img, (output.squeeze(0)*255.0).round().type(torch.uint8)))\n",
    "            \n",
    "            # save\n",
    "            output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "            if output.ndim == 3:\n",
    "                output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "            output = (output * 255.0).round().astype(np.uint8)\n",
    "            cv2.imwrite(os.path.join(save_path, img_path.split('/')[-1]), output)\n",
    "    # return scores\n",
    "            \n",
    "input_imgs = glob(\"HAT_official/datasets/data/test/lr/*.png\")\n",
    "model_paths = glob(\"HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/*.pth\")\n",
    "score_dict = {}\n",
    "for model_path in model_paths[1:]:\n",
    "    test(model_path, input_imgs, device='cuda')\n",
    "\n",
    "    \n",
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d7f0e-2e57-4fa8-a33c-6b1514175681",
   "metadata": {},
   "source": [
    "# sub image로 나눠서 inference 120x120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0937b228-063f-4415-8ea0-4779c63e8c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_60000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_40000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_75000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_50000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_45000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_10000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_65000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_70000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_30000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_85000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_25000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_20000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_55000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_35000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_80000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ../HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/net_g_90000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_img_to_tensor(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "    return img\n",
    "\n",
    "def test(model_path, input_img_paths, device='cuda'):\n",
    "    \n",
    "    metric = PSNR()\n",
    "    model = HAT(upscale=4,\n",
    "            in_chans=3,\n",
    "            img_size=64,\n",
    "            window_size=16,\n",
    "            compress_ratio=3,\n",
    "            squeeze_factor=30,\n",
    "            conv_scale=0.01,\n",
    "            overlap_ratio=0.5,\n",
    "            img_range=1.,\n",
    "            depths=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            embed_dim=180,\n",
    "            num_heads=(6,6,6,6,6,6,6,6,6,6,6,6),\n",
    "            mlp_ratio=2,\n",
    "            upsampler='pixelshuffle',\n",
    "            resi_connection='1conv')\n",
    "    \n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict['params_ema'], strict=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    save_path = os.path.join('HAT_official/datasets/data/test/inference/128', model_path.split('/')[-1][:-4])\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    \n",
    "    for img_path in tqdm(input_imgs, total=len(input_imgs), ncols=100):\n",
    "        full_img = load_img_to_tensor(img_path)\n",
    "        _, h, w = full_img.shape\n",
    "        input_img = torch.zeros((h//128*w//128, 3, 128, 128), dtype=torch.float32)\n",
    "        n = 0\n",
    "        for i in range(h//128):\n",
    "            for j in range(w//128):\n",
    "                input_img[n] += full_img[:,128*i:128*(i+1), 128*j:128*(j+1)]\n",
    "                n+=1\n",
    "                \n",
    "        input_img = input_img.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sub_output = model(input_img)\n",
    "            # scores.append(metric(hr_img, (output.squeeze(0)*255.0).round().type(torch.uint8)))\n",
    "            \n",
    "            # save\n",
    "            sub_output = sub_output.cpu()\n",
    "            output = torch.zeros((3,2048,2048), dtype=torch.float32)\n",
    "            for i in range(sub_output.shape[0]):\n",
    "                h = i // 4\n",
    "                w = i % 4\n",
    "                output[:, 512*h:512*(h+1), 512*w:512*(w+1)] += sub_output[i]\n",
    "            \n",
    "            output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "            if output.ndim == 3:\n",
    "                output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "            output = (output * 255.0).round().astype(np.uint8)\n",
    "            cv2.imwrite(os.path.join(save_path, img_path.split('/')[-1]), output)\n",
    "    # return scores\n",
    "            \n",
    "input_imgs = glob(\"HAT_official/datasets/data/test/lr/*.png\")\n",
    "model_paths = glob(\"HAT_official/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models/*.pth\")\n",
    "for model_path in model_paths[1:]:\n",
    "    print('start', model_path)\n",
    "    test(model_path, input_imgs, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8c85b-ef3f-49ab-95f3-c977f4a6bfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9ec99fb-00c0-4e01-93b8-048658e6e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@16478.447] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('../HAT_official/datasets/data/test/inference/../HAT_official/datasets/data/test/inference/test_net_g_60000.pth.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../HAT_official/datasets/data/test/inference/test_net_g_60000.pth.png'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_img = cv2.imread(os.path.join(output_path, inf_img_name), cv2.IMREAD_COLOR)\n",
    "inf_img\n",
    "inf_img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c8282cd-3cde-4df0-8694-05b2ca873938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_net_g_60000.pth.png tensor(28.8036)\n",
      "test_net_g_45000.pth.png tensor(29.1339)\n",
      "test_net_g_55000.pth.png tensor(29.0801)\n",
      "test_net_g_30000.pth.png tensor(29.2564)\n",
      "test_net_g_40000.pth.png tensor(29.0355)\n",
      "test_net_g_25000.pth.png tensor(29.1738)\n",
      "test_net_g_85000.pth.png tensor(28.8511)\n",
      "test_net_g_50000.pth.png tensor(29.0925)\n",
      "test_net_g_70000.pth.png tensor(28.8634)\n",
      "test_net_g_80000.pth.png tensor(29.0505)\n",
      "test_net_g_15000.pth.png tensor(29.3265)\n",
      "test_net_g_75000.pth.png tensor(28.9665)\n",
      "test_net_g_20000.pth.png tensor(29.2145)\n",
      "test_net_g_90000.pth.png tensor(28.8833)\n",
      "test_net_g_5000.pth.png tensor(29.6976)\n",
      "test_net_g_10000.pth.png tensor(29.5264)\n",
      "test_net_g_35000.pth.png tensor(29.2765)\n",
      "test_net_g_65000.pth.png tensor(28.7833)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class PSNR:\n",
    "    \"\"\"Peak Signal to Noise Ratio\n",
    "    img1 and img2 have range [0, 255]\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"PSNR\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(img1, img2):\n",
    "        mse = torch.mean((img1 - img2) ** 2)\n",
    "        return 20 * torch.log10(255.0 / torch.sqrt(mse))\n",
    "    \n",
    "# test_img = cv2.imread(img_list[0], cv2.IMREAD_COLOR).astype(np.float32)\n",
    "metric = PSNR()\n",
    "test_img_name = img_list[0].split('/')[-1]\n",
    "test_img = cv2.imread(os.path.join('HAT_official/datasets/data', test_img_name), cv2.IMREAD_COLOR).astype(np.float32)\n",
    "test_img = torch.from_numpy(np.transpose(test_img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n",
    "for inf_img_name in glob(f\"{output_path}/*.png\"):\n",
    "    inf_img = cv2.imread(os.path.join(inf_img_name), cv2.IMREAD_COLOR).astype(np.float32)\n",
    "    inf_img = torch.from_numpy(np.transpose(inf_img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "\n",
    "    print(inf_img_name.split('/')[-1], metric(test_img, inf_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "971145fe-4412-4df1-a75c-eb6a2e803447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = img_list[0]\n",
    "device = 'cuda'\n",
    "img = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "img = img.unsqueeze(0).to(device)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0218ed0-38a2-496e-918a-9d56bcd6b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c00033-2fe8-4d5e-8444-217dbef772bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2048, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63cec2b9-c2b5-4362-8b4b-6b58ddf0e526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2048, 2048)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cf15f09-faef-4568-8649-4f53edee0986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 2048, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if output.ndim == 3:\n",
    "    output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ddf9231-dcc8-4aa6-9da3-3f4bf2eb1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf97cea1-2e4b-463a-9674-f8cb0138fedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'HAT_official/datasets/data/test/inference'\n",
    "output = (output * 255.0).round().astype(np.uint8)\n",
    "cv2.imwrite(os.path.join(output_path, f'test.png'), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46d513-4c1d-4951-ad4e-db6c2f34a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for idx, path in enumerate(img_list):\n",
    "    # read image\n",
    "    imgname = path.split('/')[-1]\n",
    "    print('Testing', idx, imgname)\n",
    "    # read image\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float() # BGR -> RGB\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        output = model(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
